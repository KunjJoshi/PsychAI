{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers peft accelerate bitsandbytes sacremoses sentencepiece ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T08:19:44.745627Z","iopub.execute_input":"2024-06-03T08:19:44.746265Z","iopub.status.idle":"2024-06-03T08:20:04.047848Z","shell.execute_reply.started":"2024-06-03T08:19:44.746234Z","shell.execute_reply":"2024-06-03T08:20:04.046990Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.1)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: sacremoses, bitsandbytes, peft\nSuccessfully installed bitsandbytes-0.43.1 peft-0.11.1 sacremoses-0.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:44:16.568182Z","iopub.execute_input":"2024-06-03T08:44:16.568925Z","iopub.status.idle":"2024-06-03T08:44:16.874095Z","shell.execute_reply.started":"2024-06-03T08:44:16.568893Z","shell.execute_reply":"2024-06-03T08:44:16.873221Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a5d8eda90b45c0acab59d48bce5c30"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\nimport torch\nmodel_name='mistralai/Mistral-7B-v0.1'\npeft_name='Garsa3112/MentalDoctorMistral'","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:44:27.558132Z","iopub.execute_input":"2024-06-03T08:44:27.558803Z","iopub.status.idle":"2024-06-03T08:44:34.852240Z","shell.execute_reply.started":"2024-06-03T08:44:27.558776Z","shell.execute_reply":"2024-06-03T08:44:34.851233Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:44:34.854013Z","iopub.execute_input":"2024-06-03T08:44:34.854424Z","iopub.status.idle":"2024-06-03T08:44:34.860813Z","shell.execute_reply.started":"2024-06-03T08:44:34.854398Z","shell.execute_reply":"2024-06-03T08:44:34.859573Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model=AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, load_in_8bit=True)\ntokenizer=AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:44:34.862015Z","iopub.execute_input":"2024-06-03T08:44:34.862305Z","iopub.status.idle":"2024-06-03T08:46:41.052135Z","shell.execute_reply.started":"2024-06-03T08:44:34.862282Z","shell.execute_reply":"2024-06-03T08:46:41.051284Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8fc924419f442388f32ec215943a18"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a5a44034ce4ab3a5740b7939b349ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb506a4eea44bfe8bd6992f33662737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c845476926054b6989b2b0f15ac11e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4433278fcb20405fa9694751f221e41e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144022bf440a45669f7b1bbec902591d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c437d530938747968cf6b522761aa912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac4a5d24ae44b0f9c11887d58e29ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99007a4cbe4045abaf98bb634fc73af8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3170fe729f4144bf48c4b5118f8111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0252851e1304644b4118948d613755e"}},"metadata":{}}]},{"cell_type":"code","source":"peft_config=PeftConfig.from_pretrained(peft_name)\nmodel=PeftModel.from_pretrained(model, peft_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:49:32.327191Z","iopub.execute_input":"2024-06-03T08:49:32.328047Z","iopub.status.idle":"2024-06-03T08:49:33.285344Z","shell.execute_reply.started":"2024-06-03T08:49:32.328005Z","shell.execute_reply":"2024-06-03T08:49:33.284263Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/600 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e72287e7c1e4d5d89379436128330b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9763d85ea30241cab9b32cbca30e56a6"}},"metadata":{}}]},{"cell_type":"code","source":"text=\"\"\"\nMy boss seems to have a personal vendetta against me and does not provide me with promotion. He gave promotion to others but not me, not even a salary hike. \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:59:54.299672Z","iopub.execute_input":"2024-06-03T08:59:54.300044Z","iopub.status.idle":"2024-06-03T08:59:54.304667Z","shell.execute_reply.started":"2024-06-03T08:59:54.300016Z","shell.execute_reply":"2024-06-03T08:59:54.303734Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"prompt=f'I am suffering from a mental problem and facing these issues: \\n{text} \\nPlease provide me with a solution. \\nSolution:'\nimport gc\ninput_ids=tokenizer(prompt, return_tensors='pt').to(model.device)\nwith torch.no_grad():\n    generated_ids=model.generate(**input_ids, num_beams=5, max_new_tokens=512, top_p=0.9, temperature=0.6, no_repeat_ngram_size=2)\noutput=tokenizer.decode(generated_ids[0], skip_special_tokens=True)\ndel generated_ids, input_ids\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T08:59:56.591235Z","iopub.execute_input":"2024-06-03T08:59:56.591837Z","iopub.status.idle":"2024-06-03T09:00:39.125940Z","shell.execute_reply.started":"2024-06-03T08:59:56.591807Z","shell.execute_reply":"2024-06-03T09:00:39.125055Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"2150"},"metadata":{}}]},{"cell_type":"code","source":"output=output.split('Solution:')[-1]","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:00:39.127999Z","iopub.execute_input":"2024-06-03T09:00:39.128300Z","iopub.status.idle":"2024-06-03T09:00:39.132742Z","shell.execute_reply.started":"2024-06-03T09:00:39.128275Z","shell.execute_reply":"2024-06-03T09:00:39.131889Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(f'Question: {text} \\nAnswer: {output}')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:00:39.133736Z","iopub.execute_input":"2024-06-03T09:00:39.134042Z","iopub.status.idle":"2024-06-03T09:00:39.144709Z","shell.execute_reply.started":"2024-06-03T09:00:39.134018Z","shell.execute_reply":"2024-06-03T09:00:39.143779Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Question: \nMy boss seems to have a personal vendetta against me and does not provide me with promotion. He gave promotion to others but not me, not even a salary hike. \n \nAnswer: \nHello, and thank you for your question. It sounds like your boss is not giving you the promotion or salary increase that you feel you deserve. This can be very frustrating, especially if you have been working hard and feel like you are not getting the recognition and compensation that is due to you. I would encourage you to talk to your supervisor about your concerns and ask if there is anything you can do to improve your performance and increase your chances of getting a promotion and/or a raise. If you don't get a positive response, it may be time to look for a new job.\n","output_type":"stream"}]}]}